{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainCancerDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load the data\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Drop rows with missing OS and the ID column\n",
    "        self.data['OS'] = pd.to_numeric(self.data['OS'], errors='coerce')\n",
    "        self.data = self.data.dropna(subset=['OS'])\n",
    "        self.data = self.data.drop('ID', axis=1)\n",
    "        \n",
    "        self.column_order = self.data.columns\n",
    "        \n",
    "        # Separate columns into numeric and categorical\n",
    "        #self.feature_columns = [col for col in self.data.columns if col not in ['status', 'OS']]\n",
    "        self.feature_columns = [col for col in self.data.columns if col not in ['OS']]\n",
    "        numeric_cols = self.data[self.feature_columns].select_dtypes(include=['number']).columns.tolist()\n",
    "        categorical_cols = self.data[self.feature_columns].select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        self.feature_columns = numeric_cols\n",
    "        self.categorical_columns = categorical_cols\n",
    "        \n",
    "        # Initialize scalers and encoders as attributes\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        # Normalize numeric features using MinMaxScaler\n",
    "        self.features = pd.DataFrame(\n",
    "            self.scaler.fit_transform(self.data[numeric_cols]),\n",
    "            columns=numeric_cols\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Normalize the OS column using MinMaxScaler\n",
    "        self.targets = pd.Series(\n",
    "            self.scaler.fit_transform(self.data[['OS']]).flatten(),  # Normalize OS using scaler\n",
    "            name='OS'\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # One-hot encode categorical features\n",
    "        if categorical_cols:\n",
    "            self.categorical_features = self.encoder.fit_transform(self.data[categorical_cols])\n",
    "        else:\n",
    "            self.categorical_features = np.zeros((len(self.data), 0))  # Handle cases with no categorical features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Numeric features\n",
    "        features = self.features.iloc[idx].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Categorical features (one-hot encoded)\n",
    "        categorical_features = self.categorical_features[idx]\n",
    "        \n",
    "        # Target (normalized OS)\n",
    "        target = self.targets.iloc[idx]\n",
    "        \n",
    "        return features, categorical_features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, categorical_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + categorical_dim + 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, condition, categorical_features):\n",
    "        condition = condition.view(-1, 1)  # Ensure condition has two dimensions\n",
    "        x = torch.cat((noise, categorical_features, condition), dim=1).float()\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim, categorical_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim + categorical_dim + 1, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features, condition, categorical_features):\n",
    "        condition = condition.view(-1, 1)  # Ensure condition has two dimensions\n",
    "        x = torch.cat((features, categorical_features, condition), dim=1).float()\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan(generator, discriminator, dataloader, gtv_index, num_epochs=5000, lr=0.0002):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for features, categorical_features, os_value in dataloader:\n",
    "            batch_size = features.size(0)\n",
    "            features, categorical_features, os_value = (\n",
    "                features.to(device).float(),\n",
    "                categorical_features.to(device).float(),\n",
    "                os_value.to(device).float()\n",
    "            )\n",
    "\n",
    "            # Generate noise and fake features\n",
    "            noise = torch.randn(batch_size, 10).to(device)\n",
    "            fake_features = generator(noise, os_value.unsqueeze(1), categorical_features)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # -----------------------\n",
    "            # Update Discriminator\n",
    "            # -----------------------\n",
    "            optimizer_d.zero_grad()\n",
    "            real_output = discriminator(features, os_value.unsqueeze(1), categorical_features)\n",
    "            fake_output = discriminator(fake_features.detach(), os_value.unsqueeze(1), categorical_features)\n",
    "\n",
    "            # Validity penalty for invalid GTV values in fake features\n",
    "            validity_penalty_d = torch.mean(torch.relu(-fake_features[:, gtv_index]))\n",
    "\n",
    "            real_loss = criterion(real_output, real_labels)\n",
    "            fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = real_loss + fake_loss + validity_penalty_d\n",
    "            d_loss.backward()  # Backpropagation for discriminator\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # -----------------------\n",
    "            # Update Generator\n",
    "            # -----------------------\n",
    "            optimizer_g.zero_grad()\n",
    "            fake_features = generator(noise, os_value.unsqueeze(1), categorical_features)  # Recreate fake features\n",
    "            fake_output = discriminator(fake_features, os_value.unsqueeze(1), categorical_features)\n",
    "\n",
    "            # Validity penalty for invalid GTV values in fake features\n",
    "            validity_penalty_g = torch.mean(torch.relu(-fake_features[:, gtv_index]))\n",
    "\n",
    "            g_loss = criterion(fake_output, real_labels) + validity_penalty_g\n",
    "            g_loss.backward(retain_graph=False)  # Backpropagation for generator\n",
    "            optimizer_g.step()\n",
    "\n",
    "        # Logging progress\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # Save the generator and discriminator weights\n",
    "    torch.save(generator.state_dict(), 'Weights/generator_weights.pth')\n",
    "    torch.save(discriminator.state_dict(), 'Weights/discriminator_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan_balanced(generator, discriminator, dataloader, num_epochs=5000, lr=0.0002, alpha=0.1, patience=50, eval_interval=100):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    best_balance_score = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for features, categorical_features, os_value in dataloader:\n",
    "            batch_size = features.size(0)\n",
    "            features, categorical_features, os_value = (\n",
    "                features.to(device).float(),\n",
    "                categorical_features.to(device).float(),\n",
    "                os_value.to(device).float()\n",
    "            )\n",
    "\n",
    "            # Generate fake features\n",
    "            noise = torch.randn(batch_size, 10).to(device)\n",
    "            fake_features = generator(noise, os_value.unsqueeze(1), categorical_features)\n",
    "\n",
    "            # Real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Discriminator Loss\n",
    "            real_loss = criterion(discriminator(features, os_value.unsqueeze(1), categorical_features), real_labels)\n",
    "            fake_loss = criterion(discriminator(fake_features.detach(), os_value.unsqueeze(1), categorical_features), fake_labels)\n",
    "\n",
    "            # Validity penalty for invalid GTV values in fake features\n",
    "            validity_penalty_d = torch.mean(torch.relu(-fake_features[:, 1]))\n",
    "\n",
    "            d_loss = real_loss + fake_loss + validity_penalty_d\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # Validity penalty for invalid GTV values in fake features\n",
    "            validity_penalty_g = torch.mean(torch.relu(-fake_features[:, 1]))\n",
    "\n",
    "            # Generator Loss\n",
    "            g_loss = criterion(discriminator(fake_features, os_value.unsqueeze(1), categorical_features), real_labels) + validity_penalty_g\n",
    "\n",
    "            optimizer_g.zero_grad()\n",
    "            g_loss.backward() \n",
    "            optimizer_g.step()\n",
    "\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_d_loss = epoch_d_loss / num_batches\n",
    "        avg_g_loss = epoch_g_loss / num_batches\n",
    "\n",
    "        # Calculate balance score (difference between generator and discriminator loss)\n",
    "        balance_score = abs(avg_d_loss - avg_g_loss)\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f} | Balance Score: {balance_score:.4f}\")\n",
    "\n",
    "        # Save models if balance improves\n",
    "        if balance_score < best_balance_score:\n",
    "            best_balance_score = balance_score\n",
    "            no_improve_epochs = 0  # Reset patience counter\n",
    "            torch.save(generator.state_dict(), 'Weights/generator_weights.pth')\n",
    "            torch.save(discriminator.state_dict(), 'Weights/discriminator_weights.pth')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best Balance Score: {best_balance_score:.4f}\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_correlation(generator, dataset):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "\n",
    "    original_features = []\n",
    "    generated_features = []\n",
    "    os_values = []\n",
    "\n",
    "    for features, categorical_features, os_value in dataset:\n",
    "        features, categorical_features, os_value = features.clone().detach().to(device), categorical_features.clone().detach().to(device), os_value.clone().detach().to(device)\n",
    "        noise = torch.randn(features.size(0), 10).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_features = generator(noise, os_value.unsqueeze(1), categorical_features)\n",
    "\n",
    "        generated_features.append(fake_features.cpu().numpy())\n",
    "        original_features.append(features.cpu().numpy())\n",
    "        os_values.append(os_value.cpu().numpy())\n",
    "\n",
    "    original_features = np.concatenate(original_features)\n",
    "    generated_features = np.concatenate(generated_features)\n",
    "    os_values = np.concatenate(os_values)\n",
    "\n",
    "    correlation_real = np.corrcoef(original_features[:, -1], os_values)[0, 1]\n",
    "    correlation_fake = np.corrcoef(generated_features[:, -1], os_values)[0, 1]\n",
    "\n",
    "    print(f\"Real Data Correlation with OS: {correlation_real:.4f}\")\n",
    "    print(f\"Synthetic Data Correlation with OS: {correlation_fake:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRecovery:\n",
    "    def __init__(self, generator, dataset, encoder, scaler):\n",
    "        self.generator = generator\n",
    "        self.dataset = dataset\n",
    "        self.encoder = encoder  # Fitted OneHotEncoder instance\n",
    "        self.scaler = scaler    # Fitted MinMaxScaler instance\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator.to(self.device)\n",
    "\n",
    "    def custom_round(self,number):\n",
    "\n",
    "        base = (number // 10) * 10  # Nearest lower multiple of 10\n",
    "        remainder = number % 10\n",
    "\n",
    "        # Check the specific cases for midpoints\n",
    "        if remainder == 1:  # e.g., 11\n",
    "            return base  # Round down\n",
    "        elif remainder == 6:  # e.g., 26\n",
    "            return base + 10  # Round up\n",
    "        else:\n",
    "            # Standard rounding\n",
    "            return base + (10 if remainder >= 5 else 0)\n",
    "\n",
    "\n",
    "    def generate_rows(self, num_rows):\n",
    "            self.generator.eval()\n",
    "            all_generated = []\n",
    "\n",
    "            for _ in range(num_rows):\n",
    "                idx = np.random.randint(len(self.dataset))\n",
    "                features, categorical_features, os_value = self.dataset[idx]\n",
    "\n",
    "                features = torch.tensor(features, dtype=torch.float32).to(self.device)\n",
    "                categorical_features = torch.tensor(categorical_features, dtype=torch.float32).to(self.device)\n",
    "                os_value = torch.tensor(os_value, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                noise = torch.randn(1, 10).to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    generated_features = self.generator(noise, os_value.view(1, -1), categorical_features.unsqueeze(0))\n",
    "\n",
    "                generated_features_np = generated_features.cpu().numpy().flatten()\n",
    "                categorical_features_np = categorical_features.cpu().numpy().reshape(1, -1)\n",
    "                os_value_np = np.array([[os_value.cpu().numpy()]])\n",
    "\n",
    "                # Reverse one-hot encoding\n",
    "                original_categorical = self.encoder.inverse_transform(categorical_features_np)\n",
    "\n",
    "                # Denormalize 'OS'\n",
    "                original_os = self.scaler.inverse_transform(os_value_np)\n",
    "\n",
    "                # Denormalize ['KI','GTV']\n",
    "                status = generated_features_np[2]\n",
    "                original_generated_features = self.scaler.inverse_transform(generated_features_np.reshape(1, -1))\n",
    "                original_generated_features[0,2] = np.round(status,0)\n",
    "\n",
    "                original_generated_features[0,0] = self.custom_round(original_generated_features[0,0])\n",
    "\n",
    "                all_generated.append(\n",
    "                    np.concatenate((\n",
    "                        original_generated_features.flatten(),\n",
    "                        original_categorical.flatten(),\n",
    "                        original_os.flatten()\n",
    "                    ))\n",
    "                )\n",
    "\n",
    "            self.generator.train()\n",
    "            return np.array(all_generated)\n",
    "\n",
    "    def save_generated(self, num_rows, column_order, filename=\"GeneratedData/Generated_Data_BrainCancer.csv\"):\n",
    "        generated_data = self.generate_rows(num_rows)\n",
    "\n",
    "        if hasattr(self.dataset, \"feature_columns\") and hasattr(self.dataset, \"categorical_columns\"):\n",
    "            columns = self.dataset.feature_columns + self.dataset.categorical_columns + ['OS']\n",
    "        else:\n",
    "            raise AttributeError(\"Dataset must have 'feature_columns' and 'categorical_columns' attributes.\")\n",
    "\n",
    "        if generated_data.shape[1] != len(columns):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between data shape ({generated_data.shape[1]} columns) and column names ({len(columns)}).\"\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(generated_data, columns=columns)\n",
    "        df = df[column_order]\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Generated data saved to {filename}\")\n",
    "\n",
    "    def view_generated(self, num_rows):\n",
    "        generated_data = self.generate_rows(num_rows)\n",
    "\n",
    "        if hasattr(self.dataset, \"feature_columns\") and hasattr(self.dataset, \"categorical_columns\"):\n",
    "            columns = self.dataset.feature_columns + self.dataset.categorical_columns + ['OS']\n",
    "        else:\n",
    "            raise AttributeError(\"Dataset must have 'feature_columns' and 'categorical_columns' attributes.\")\n",
    "\n",
    "        if generated_data.shape[1] != len(columns):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between data shape ({generated_data.shape[1]} columns) and column names ({len(columns)}).\"\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(generated_data, columns=columns)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"Data/BrainCancer.csv\"\n",
    "dataset = BrainCancerDataset(csv_file)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "feature_dim = len(dataset[0][0])\n",
    "categorical_dim = dataset.categorical_features.shape[1]\n",
    "\n",
    "generator = Generator(input_dim=10, feature_dim=feature_dim, categorical_dim=categorical_dim)\n",
    "discriminator = Discriminator(feature_dim=feature_dim, categorical_dim=categorical_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000] | D Loss: 1.3863 | G Loss: 0.6713 | Balance Score: 0.7150\n",
      "Epoch [500/100000] | D Loss: 1.3130 | G Loss: 0.7820 | Balance Score: 0.5310\n",
      "Epoch [1000/100000] | D Loss: 1.1894 | G Loss: 0.9201 | Balance Score: 0.2693\n",
      "Epoch [1500/100000] | D Loss: 1.4025 | G Loss: 0.7148 | Balance Score: 0.6877\n",
      "Epoch [2000/100000] | D Loss: 1.4442 | G Loss: 0.7261 | Balance Score: 0.7180\n",
      "Epoch [2500/100000] | D Loss: 1.2439 | G Loss: 0.8496 | Balance Score: 0.3943\n",
      "Epoch [3000/100000] | D Loss: 1.2063 | G Loss: 0.9288 | Balance Score: 0.2775\n",
      "Epoch [3500/100000] | D Loss: 1.2484 | G Loss: 0.8592 | Balance Score: 0.3892\n",
      "Epoch [4000/100000] | D Loss: 1.1949 | G Loss: 0.9146 | Balance Score: 0.2802\n",
      "Epoch [4500/100000] | D Loss: 1.1883 | G Loss: 0.7921 | Balance Score: 0.3962\n",
      "Epoch [5000/100000] | D Loss: 1.2064 | G Loss: 0.9318 | Balance Score: 0.2747\n",
      "Epoch [5500/100000] | D Loss: 1.1742 | G Loss: 0.9125 | Balance Score: 0.2617\n",
      "Epoch [6000/100000] | D Loss: 1.1591 | G Loss: 0.8844 | Balance Score: 0.2747\n",
      "Epoch [6500/100000] | D Loss: 1.1547 | G Loss: 0.9319 | Balance Score: 0.2229\n",
      "Epoch [7000/100000] | D Loss: 1.1210 | G Loss: 0.9906 | Balance Score: 0.1304\n",
      "Epoch [7500/100000] | D Loss: 1.1419 | G Loss: 0.9318 | Balance Score: 0.2101\n",
      "Epoch [8000/100000] | D Loss: 1.1007 | G Loss: 0.9401 | Balance Score: 0.1606\n",
      "Epoch [8500/100000] | D Loss: 1.1405 | G Loss: 0.9793 | Balance Score: 0.1612\n",
      "Epoch [9000/100000] | D Loss: 1.1572 | G Loss: 1.0703 | Balance Score: 0.0869\n",
      "Epoch [9500/100000] | D Loss: 1.1508 | G Loss: 1.0415 | Balance Score: 0.1093\n",
      "Epoch [10000/100000] | D Loss: 1.1178 | G Loss: 1.0080 | Balance Score: 0.1098\n",
      "Epoch [10500/100000] | D Loss: 1.1258 | G Loss: 1.1340 | Balance Score: 0.0083\n",
      "Epoch [11000/100000] | D Loss: 1.1355 | G Loss: 0.8863 | Balance Score: 0.2491\n",
      "Epoch [11500/100000] | D Loss: 1.1450 | G Loss: 1.0139 | Balance Score: 0.1311\n",
      "Epoch [12000/100000] | D Loss: 1.1478 | G Loss: 1.0174 | Balance Score: 0.1304\n",
      "Epoch [12500/100000] | D Loss: 1.1331 | G Loss: 1.0457 | Balance Score: 0.0874\n",
      "Epoch [13000/100000] | D Loss: 1.0856 | G Loss: 0.9864 | Balance Score: 0.0992\n",
      "Epoch [13500/100000] | D Loss: 1.1013 | G Loss: 1.0811 | Balance Score: 0.0201\n",
      "Epoch [14000/100000] | D Loss: 1.0846 | G Loss: 1.1058 | Balance Score: 0.0212\n",
      "Epoch [14500/100000] | D Loss: 1.0886 | G Loss: 1.2202 | Balance Score: 0.1316\n",
      "Epoch [15000/100000] | D Loss: 1.1150 | G Loss: 1.3465 | Balance Score: 0.2315\n",
      "Epoch [15500/100000] | D Loss: 1.0847 | G Loss: 1.2287 | Balance Score: 0.1440\n",
      "Epoch [16000/100000] | D Loss: 1.0895 | G Loss: 1.1519 | Balance Score: 0.0623\n",
      "Epoch [16500/100000] | D Loss: 1.0931 | G Loss: 1.1345 | Balance Score: 0.0415\n",
      "Epoch [17000/100000] | D Loss: 1.0762 | G Loss: 1.0825 | Balance Score: 0.0062\n",
      "Epoch [17500/100000] | D Loss: 1.0860 | G Loss: 1.0855 | Balance Score: 0.0005\n",
      "Epoch [18000/100000] | D Loss: 1.0319 | G Loss: 1.2769 | Balance Score: 0.2450\n",
      "Epoch [18500/100000] | D Loss: 1.0830 | G Loss: 1.1453 | Balance Score: 0.0623\n",
      "Epoch [19000/100000] | D Loss: 1.0498 | G Loss: 1.2355 | Balance Score: 0.1858\n",
      "Epoch [19500/100000] | D Loss: 1.0509 | G Loss: 1.1940 | Balance Score: 0.1431\n",
      "Epoch [20000/100000] | D Loss: 1.0704 | G Loss: 1.2204 | Balance Score: 0.1501\n",
      "Epoch [20500/100000] | D Loss: 1.0237 | G Loss: 1.3553 | Balance Score: 0.3316\n",
      "Epoch [21000/100000] | D Loss: 1.0428 | G Loss: 1.3955 | Balance Score: 0.3528\n",
      "Epoch [21500/100000] | D Loss: 1.1087 | G Loss: 1.2559 | Balance Score: 0.1471\n",
      "Epoch [22000/100000] | D Loss: 1.0430 | G Loss: 1.4667 | Balance Score: 0.4237\n",
      "Early stopping at epoch 22311. Best Balance Score: 0.0000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#train_cgan(generator, discriminator, dataloader, gtv_index=1, num_epochs=10000)\n",
    "train_cgan_balanced(generator, discriminator, dataloader,  num_epochs=100000, patience=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Data Correlation with OS: -0.4022\n",
      "Synthetic Data Correlation with OS: -0.4917\n"
     ]
    }
   ],
   "source": [
    " # Load the saved weights (optional, if you want to reuse the models)\n",
    "generator.load_state_dict(torch.load('best_generator_weights.pth'))\n",
    "discriminator.load_state_dict(torch.load('best_discriminator_weights.pth'))\n",
    "\n",
    "# Validate the model\n",
    "validate_correlation(generator, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data saved to GeneratedData/generated_data.csv\n",
      "Generated DataFrame:\n",
      "      KI        GTV status     Sex   Diagnosis        Location  \\\n",
      "0  60.0  48.656315    1.0    male  meningioma  supratentorial   \n",
      "1  60.0  23.922226    0.0    male   lg_glioma  supratentorial   \n",
      "2  60.0  13.117501    0.0    male  meningioma  supratentorial   \n",
      "3  50.0  18.215059    1.0  female   lg_glioma  supratentorial   \n",
      "4  80.0  61.237827    0.0    male       other  supratentorial   \n",
      "\n",
      "  Stereotactic methods         OS  \n",
      "0                  srt  31.147541  \n",
      "1                  srt  35.934425  \n",
      "2                  srt  67.377045  \n",
      "3                  srt   47.80328  \n",
      "4                  srt  19.737705  \n"
     ]
    }
   ],
   "source": [
    "# Data recovery\n",
    "data_recovery = DataRecovery(generator, dataset, dataset.encoder, dataset.scaler)\n",
    "\n",
    "# Generate synthetic rows\n",
    "num_rows = 50000\n",
    "generated_rows = data_recovery.generate_rows(num_rows)\n",
    "#print(\"Generated Rows:\\n\", generated_rows)\n",
    "\n",
    "# Save synthetic rows to a CSV file\n",
    "data_recovery.save_generated(num_rows=num_rows, column_order=dataset.column_order , filename=\"GeneratedData/Generated_Data_BrainCancer.csv\")\n",
    "\n",
    "# View synthetic rows as a pandas DataFrame\n",
    "df = data_recovery.view_generated(num_rows=num_rows)\n",
    "print(\"Generated DataFrame:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
